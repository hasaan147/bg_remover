{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93cb5348-a2e0-429c-ac1a-c3fdca9a78e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: pillow in c:\\users\\shekhani laptops\\appdata\\roaming\\python\\python312\\site-packages (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shekhani laptops\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shekhani laptops\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shekhani laptops\\appdata\\roaming\\python\\python312\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\shekhani laptops\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (1.26.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shekhani laptops\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision opencv-python pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71cb4a19-bc18-4bf6-842c-86ac5e21c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39881673-bed1-42af-844a-a2696aea7a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to C:\\Users\\Shekhani Laptops/.cache\\torch\\hub\\checkpoints\\deeplabv3_resnet101_coco-586e9e4e.pth\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 233M/233M [23:12<00:00, 176kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af64c44-30be-4727-8f8a-ebb5a76c0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f2b0d1-1916-4155-9565-00b5defcdade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_background(image_path):\n",
    "    # Load the image\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Move to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)['out'][0]\n",
    "    \n",
    "    # Get the mask (Class 0 is background, Class 15 is person)\n",
    "    output_predictions = output.argmax(0).byte().cpu().numpy()\n",
    "    \n",
    "    # Convert the mask into a 3-channel image\n",
    "    mask = output_predictions == 15  # Assuming class 15 represents the person\n",
    "    mask = np.stack([mask]*3, axis=-1)\n",
    "    \n",
    "    # Convert the PIL image to a NumPy array\n",
    "    image_np = np.array(input_image)\n",
    "    \n",
    "    # Apply the mask to remove the background\n",
    "    result_image = image_np * mask\n",
    "    \n",
    "    # Convert the result to an image and save it\n",
    "    result = Image.fromarray(result_image.astype(np.uint8))\n",
    "    result.show()  # Display the image\n",
    "    result.save('output.png')  # Save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9079e933-04f1-4f92-840c-f444839b557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'test.jpeg'\n",
    "remove_background(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88557871-1046-4553-9b6c-1b82406d6dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '1.jpeg'\n",
    "remove_background(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52a4a6-6b97-4142-bb3b-91c569e4a59d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
